sudo apt update
sudo apt upgrade
sudo apt install python3
python3 --version
sudo apt install python3-pip
pip3 list

* POR SI FALLA EL VSCODE JUPYTER NOTEBOOK
sudo apt update
sudo apt install python3-ipykernel

* JAVA INSTALLATION
sudo apt-get install openjdk-8-jdk

java -version
javac -version

sudo apt-get install ssh

sudo apt  install gedit

* PARA CONFIGURAR LAS VARIABLES DE ENTORNO
gedit ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_HOME=/home/hadoop/hadoop-3.3.6
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh localhost
chmod 0600 ~/.ssh/authorized_keys

hdfs-site.xml
<property>
        	<name>dfs.replication</name>
        	<value>1</value>
    	</property>
    	<property>
            <name>dfs.namenode.name.dir</name>
            <value>file:/home/hadoop/hadoop-3.3.6/data/namenode</value>
     	</property>
     	<property>
            <name>dfs.datanode.data.dir</name>
            <value>file:/home/hadoop/hadoop-3.3.6/data/datanode</value>
     	</property>

core-site.xml
<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
    	</property>

mapred-site.xml
<property>
        	<name>mapreduce.framework.name</name>
        	<value>yarn</value>
    	</property>
    	<property>
        	<name>mapreduce.application.classpath</name>
        	<value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    	</property>

yarn-site.xml
<property>
        	<name>yarn.nodemanager.aux-services</name>
        	<value>mapreduce_shuffle</value>
    	</property>
    	<property>
        <name>yarn.nodemanager.env-whitelist</name>
      <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
    </property>

hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

* CREAR LA CARPETA data

* hdfs namenode -format

start-all.sh
start-dfs.sh

jps

stop-all.sh

sudo apt install net-tools

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_HOME=/home/hadoop/hadoop-3.3.6
export SPARK_HOME=/home/hadoop/spark-3.5.3-bin-hadoop3
export PYSPARK_PYTHON=python3
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$PATH

en spark.conf
spark.hadoop.fs.defaultFS=hdfs://localhost:9001

sudo apt install python3.12-venv
python3 -m venv my-venv
source my-venv/bin/activate


*** HIVE
COPIAR EL GUAVA JAR DESDE HADOOP A HIVE CONF LIB  (BORRAR EL GUAVA JAR DE HIVE)
utilizar hive 3.1.3 compatible con spark 3.5.3

CONFIGURAR hive-site.xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://localhost:9083</value> <!-- Cambia según tu configuración si es necesario -->
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>0.0.0.0</value> <!-- O usa 'localhost' según tu necesidad -->
    </property>
    	<property>
    		<name>hive.server2.enable.doAs</name>
    		<value>false</value>
	</property>
</configuration>

CONFIGURAR hive-env.sh
export HADOOP_HOME=/home/hadoop/hadoop-3.3.6
export HIVE_HOME=/home/hadoop/apache-hive-3.1.3-bin
export HIVE_CONF_DIR=$HIVE_HOME/conf

CONFIGURAR spark-defaults.conf
spark.hadoop.fs.defaultFS=hdfs://localhost:9001
spark.sql.hive.metastore.version=3.1.3
spark.sql.hive.metastore.jars=/home/hadoop/apache-hive-3.1.3-bin/lib/*
spark.sql.hive.metastore.uris=thrift://127.0.0.1:9083

EJECUTAR:
schematool -initSchema -dbType derby

EN EL HDFS USAR:
hdfs dfs -mkdir /tmp
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hive
hdfs dfs -mkdir /user/hive/warehouse
hdfs dfs -chmod 1777 /tmp


EN HADOOP core-site.xml PONER
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9001</value>
    	</property>
    	<property>
        	<name>hadoop.proxyuser.hadoop.hosts</name>
        	<value>*</value>
    	</property>
    	<property>
        	<name>hadoop.proxyuser.hadoop.groups</name>
        	<value>*</value>
    	</property>
</configuration>

para salir de BEELINE !quit

SI ALGUN SERVICIO DE HIVE ESTA CORRIENDO DETENERLO
ps aux | grep Hive
kill -9 9695

INICIAR SERVICIOS DE HIVE
hive --service metastore &
hive --service hiveserver2 &

INICIAR HIVE
beeline -u jdbc:hive2://localhost:10000/default

IP DE LA VM
192.168.1.106
